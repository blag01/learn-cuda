
CUDA is a (frustrating yet) fun experience. I love it.

- special cases for n = 0 and n = 1 can catch you when doing reductions
- It’s annoying to have to reach for cudaMemcpy when all you want to do is move a value from one pointer into another (*dst = *src). I am not sure why it’s one call instead of 4 dynamically dispatched based on the last arg though. nevermind
- implemented sorting by iteratively merging already sorted subsequences. Seemed the most intuitive way to go about it. Looked at what others do - bitonic sorting seems to be the keyword here. Don’t understand it yet
- sometimes an exercise calls for a kernel but I end up doing a sequence of kernels calls instead that build up on each other sequentially (e.g. y = 2 * x, z = exp(y), w = z * z). I am not sure if this can be package as a kernel by itself somehow (sequence of kernels -> kernel of ?? Sequences ??) 
- I admit … I almost forgot to cudaFree my buffers. Got to look up RAII in cuda context. It feels like I’m writing C and I’m scared to reach for anything more extreme - exceptions? heap allocations with new? std (admittedly, I’m doing exercises that discourage the use of external libraries so I’m sure I’ll get my education on that aspect of cuda at some point) 
- apparently, shared memory is not zeroed out. I resorted to manually zeroing it out (cudaMemset or equivalent) and then __syncthreads() to ensure no threads proceeds to use it before it’s ready. Not sure this is optimal?
- I find myself doing important algorithmic steps only on thread 0 for each block (especially when I’ve been doing things with per block shared memory). Not sure if that’s a pattern or an anti-pattern? semes to be ok if the shared memory is accumulation of results from the whole block. however, if it's an array ...
- ... then another pattern helps: have each thread in a block handle the shared memory indices whose modulo corresponds to the threadId (e.g. shared memory of size 100, 16 threads, thread 2 handles 2, 18, 34, etc.). this is better than having a loop only on thread 0 (avoid divergence and is obviously faster in wall time)
- my intution for when shared memory will help sucks - i benchmarked atomicAdd from each thread into a global address vs atomicAdd-ing into a block-shared memory first and then atomicAdding once per block into the global address - essentially the same? yet, in other cases it helps massively
- i think part of the reason is that i don't use float4, int4, etc. and instead default to 1 element per thread processing. if memory is fetched in 128-bit chunks then it's probably faster to have one thread read all these values rather than 4...
- pretty sure at this point i can write "threadIdx.x + blockIdx.x * blockDim.x" even if unconscious