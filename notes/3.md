
- silent failures when launching too many threads - easy mistake to make with 3d indexes especially
- several times already I've written the equivalent of blockDim.x * blockDim.x or blockIdx.x * blockIdx.x - very annoying
- started reading a bit more about the way GPUs work. streaming multiprocessors (SM) is an important concept to understand as well as how warps work under the hood (current understanding: a giant SIMD instruction spanning 4 floats x warp size (i.e. 32) - this is why using int4, float4 and covering 4 elements at once in a kernel leads to improvements)
- i find myself reaching for cudaMemcpy, cudaMemset, etc. more and more; when i need to sequentially launch kernels that depend on the result of the previous kernel (e.g. working in waves, etc.) i sometimes want to grab a GPU-stored value, modify a buffer before the next wave, etc. I see people writing small kernels for these things but in my mind a kernel is still too heavy for what's essentially std::fill? but on the other hand, it could well be that for a device -> host -> device roundtrip, cudaMemcpy might well be slower than a kernel. Still, especially when working with scalars, 1x1 kernel feels wrong and abusive of the hardware.
- especially important in 2D, 3D tasks: must must must flip the order of coordinates to ensure threads in a warp read consecutive memory. this is because (in row-major representation) indices are x * #cols + y and each warp shares the same threadIdx.x. so cuda x must be mapped to logical y.
- i now understand that i don't understand when i need cudaDeviceSynchronize() in host code? can something go wrong if I launch kernels one after the other if I don't call that? need to read docs...
- sadly, i don't see how (safe) Rust and cuda cooperate nicely. C(++) really is a much more suitable language to deal with pointers, parallelism and constant io. nothing about cuda kernels is safe and nothing can be proven to the borrow checker