Decided to play with CUDA for real today.

Uhm, my head hurts. Spent 3 hours debugging a "basic" reduction which introduced me to ten different types of gotchas.

- reading the result from a device pointer on the host without a cudaMemcpy - bad. Big gotcha that the pointers have "colors" now - device/host and you have to implicitly remember which is which somehow - type is not enough.
- ping / pong buffers and shared memory - took a while to get the intuition for what's going on and how shared memories interacts with blocks / warps / __syncthreads() etc
- speaking of shared memory - super easy to mess up the dynamic allocations when using the extern version (rather than a static one)
- very hard to debug: printf not reliable when something goes horribly wrong (read next)
- i tried to be a smart ass and write a generic reduce function that accepted a (float, float) -> float callback. Nope. Everything breaks. Still no clue why? Even when i pass in a __device__ annotated func. Hardcoding it works, passing it via callback does not. I figured the type might not be float (*fn)(float, float) for a device function so I templated the callback type and still no luck :/
- it’s a bit annoying that the order of specification is blocks, threads (at least for me as a noob anyway)
- i feel very conflicted about writing C++ again but whatever. Even more conflicted with the naming conventions as I'm used to snake case in C/C++ and cuda uses camelCase.
- memory access pattern is everything. it's insane how much of a difference this makes. on CPU it matters too but in CUDA it's next level
- the above is why understanding warps really begins to matter when using 2d and 3d indexing
- everyone says „gpus make matmul fast“ but the naive version of parallelizing individual dot products is far from optimal. Still haven’t fully understood tiled matmul
- recursive kernels? kernel called from inside another? feels naughty 
- i still suspect there must be an easier way to find a max of an array to doing all of this jiu-jitsu with shared memory / ping-pong buffers, etc.